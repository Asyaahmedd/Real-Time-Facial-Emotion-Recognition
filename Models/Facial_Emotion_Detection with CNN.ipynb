{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Copy of Salah_Hussien(CNN ARCH1/2).ipynb",
      "provenance": [],
      "collapsed_sections": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "code",
      "source": [
        "#mount our drive to load data \n",
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "xgqzaIsOtn6R",
        "outputId": "b3f2a3e5-0852-4dd5-a76b-a9bb153b4218"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mounted at /content/drive\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#imports library \n",
        "import pandas as pd\n",
        "import sklearn\n",
        "import keras\n",
        "import tensorflow as tf\n",
        "import tensorflow.keras\n",
        "import matplotlib.pyplot as plt\n",
        "import random\n",
        "import os\n",
        "import cv2 \n",
        "import seaborn as sns \n",
        "import matplotlib.pyplot as plt\n",
        "import numpy as np\n",
        "import skimage \n",
        "from skimage import io,color\n",
        "from skimage.io import imread,imshow\n",
        "from skimage.color import rgb2gray,gray2rgb\n",
        "from skimage.transform import  resize, rescale\n",
        "from sklearn import datasets, metrics, model_selection,preprocessing\n",
        "from sklearn.metrics import confusion_matrix,mean_squared_error,classification_report,ConfusionMatrixDisplay,roc_curve,roc_auc_score\n",
        "from sklearn.model_selection import train_test_split\n",
        "from keras.layers import Dense, Dropout, Flatten,Dense,Input\n",
        "from keras.models import Model,Sequential\n",
        "from keras.applications.vgg16 import VGG16, preprocess_input\n",
        "from keras.preprocessing.image import ImageDataGenerator\n",
        "from keras.callbacks import ModelCheckpoint, EarlyStopping\n",
        "from tensorflow.keras.optimizers import Adam \n",
        "from tensorflow.keras.applications import VGG16, VGG19\n",
        "from tensorflow.keras import regularizers\n",
        "from tensorflow.keras.models import Model\n",
        "from tensorflow.keras.layers import Dense, Flatten, Conv2D, MaxPooling2D,BatchNormalization, Activation\n",
        "from pathlib import Path\n",
        "from math import sqrt"
      ],
      "metadata": {
        "id": "Z4GVMHu0pINm"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def load_dataset(PathOfSavedData):\n",
        "  #load images as numpy array , and make splitting train, validation \n",
        "  X_train = np.load(PathOfSavedData+'X_train1.npy', mmap_mode='r')\n",
        "  y_train = np.load(PathOfSavedData+'y_train1.npy', mmap_mode='r')\n",
        "  X_valid = np.load(PathOfSavedData+'X_valid1.npy', mmap_mode='r')\n",
        "  y_valid = np.load(PathOfSavedData+'y_valid1.npy', mmap_mode='r')\n",
        "\n",
        "  \n",
        "  X_train=X_train['arr_0']\n",
        "  y_train=y_train['arr_0']\n",
        "  X_valid=X_valid['arr_0']\n",
        "  y_valid=y_valid['arr_0']\n",
        "\n",
        "  return (X_train,y_train,X_valid,y_valid)\n",
        "#call the load function\n",
        "X_train,y_train,X_valid,y_valid = load_dataset(\"/content/drive/MyDrive/EmotionDataset/\")"
      ],
      "metadata": {
        "id": "SBqlasTA0mXj"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#label encoding \n",
        "mapping = {\"angry\":0, \"happy\":1,\"neutral\":2, \"sad\":3, \"surprise\":4}\n",
        "y_train_class = np.array([mapping[key] for key in y_train])\n",
        "y_valid_class = np.array([mapping[key] for key in y_valid])\n",
        "\n",
        "nClasses = 5\n",
        "# Use Keras' handy utils\n",
        "y_train_k = tensorflow.keras.utils.to_categorical(y_train_class, num_classes=nClasses)\n",
        "y_valid_k = tensorflow.keras.utils.to_categorical(y_valid_class, num_classes=nClasses)"
      ],
      "metadata": {
        "id": "gA_xIfyStWFw"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#fisrt architecture   \n",
        "def my_model():\n",
        "  #start sequential model\n",
        "  model= tf.keras.models.Sequential()\n",
        "  model.add(Conv2D(32, kernel_size=(3, 3), padding='same', activation='relu', input_shape=(92, 112,1)))\n",
        "  model.add(Conv2D(64,(3,3), padding='same', activation='relu' ))\n",
        "  model.add(BatchNormalization())\n",
        "  model.add(MaxPooling2D(pool_size=(2, 2)))\n",
        "  model.add(Dropout(0.25))\n",
        "\n",
        "  model.add(Conv2D(128,(5,5), padding='same', activation='relu'))\n",
        "  model.add(BatchNormalization())\n",
        "  model.add(MaxPooling2D(pool_size=(2, 2)))\n",
        "  model.add(Dropout(0.25))\n",
        "      \n",
        "  model.add(Conv2D(512,(3,3), padding='same', activation='relu', kernel_regularizer=regularizers.l2(0.01)))\n",
        "  model.add(BatchNormalization())\n",
        "  model.add(MaxPooling2D(pool_size=(2, 2)))\n",
        "  model.add(Dropout(0.25))\n",
        "\n",
        "  model.add(Conv2D(512,(3,3), padding='same', activation='relu', kernel_regularizer=regularizers.l2(0.01)))\n",
        "  model.add(BatchNormalization())\n",
        "  model.add(MaxPooling2D(pool_size=(2, 2)))\n",
        "  model.add(Dropout(0.25))\n",
        "\n",
        "  model.add(Flatten()) \n",
        "  model.add(Dense(256,activation = 'relu'))\n",
        "  model.add(BatchNormalization())\n",
        "  model.add(Dropout(0.25))\n",
        "      \n",
        "  model.add(Dense(512,activation = 'relu'))\n",
        "  model.add(BatchNormalization())\n",
        "  model.add(Dropout(0.25))\n",
        "\n",
        "  model.add(Dense(5, activation='softmax'))\n",
        "\n",
        "  model.compile(\n",
        "      optimizer = Adam(lr=0.0001), \n",
        "      loss='categorical_crossentropy', \n",
        "      metrics=['accuracy']\n",
        "    )\n",
        "\n",
        "  return model"
      ],
      "metadata": {
        "id": "84wPuo5yOdwh"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#second architecture\n",
        "# def my_model(input_shape=(48,48,1)):\n",
        "#     # first input model\n",
        "#     visible = Input(shape=input_shape, name='input')\n",
        "#     num_classes = 7\n",
        "#     #the 1-st block\n",
        "#     conv1_1 = Conv2D(64, kernel_size=3, activation='relu', padding='same', name = 'conv1_1')(visible)\n",
        "#     conv1_1 = BatchNormalization()(conv1_1)\n",
        "#     conv1_2 = Conv2D(64, kernel_size=3, activation='relu', padding='same', name = 'conv1_2')(conv1_1)\n",
        "#     conv1_2 = BatchNormalization()(conv1_2)\n",
        "#     pool1_1 = MaxPooling2D(pool_size=(2,2), name = 'pool1_1')(conv1_2)\n",
        "#     drop1_1 = Dropout(0.3, name = 'drop1_1')(pool1_1)#the 2-nd block\n",
        "#     conv2_1 = Conv2D(128, kernel_size=3, activation='relu', padding='same', name = 'conv2_1')(drop1_1)\n",
        "#     conv2_1 = BatchNormalization()(conv2_1)\n",
        "#     conv2_2 = Conv2D(128, kernel_size=3, activation='relu', padding='same', name = 'conv2_2')(conv2_1)\n",
        "#     conv2_2 = BatchNormalization()(conv2_2)\n",
        "#     conv2_3 = Conv2D(128, kernel_size=3, activation='relu', padding='same', name = 'conv2_3')(conv2_2)\n",
        "#     conv2_2 = BatchNormalization()(conv2_3)\n",
        "#     pool2_1 = MaxPooling2D(pool_size=(2,2), name = 'pool2_1')(conv2_3)\n",
        "#     drop2_1 = Dropout(0.3, name = 'drop2_1')(pool2_1)#the 3-rd block\n",
        "#     conv3_1 = Conv2D(256, kernel_size=3, activation='relu', padding='same', name = 'conv3_1')(drop2_1)\n",
        "#     conv3_1 = BatchNormalization()(conv3_1)\n",
        "#     conv3_2 = Conv2D(256, kernel_size=3, activation='relu', padding='same', name = 'conv3_2')(conv3_1)\n",
        "#     conv3_2 = BatchNormalization()(conv3_2)\n",
        "#     conv3_3 = Conv2D(256, kernel_size=3, activation='relu', padding='same', name = 'conv3_3')(conv3_2)\n",
        "#     conv3_3 = BatchNormalization()(conv3_3)\n",
        "#     conv3_4 = Conv2D(256, kernel_size=3, activation='relu', padding='same', name = 'conv3_4')(conv3_3)\n",
        "#     conv3_4 = BatchNormalization()(conv3_4)\n",
        "#     pool3_1 = MaxPooling2D(pool_size=(2,2), name = 'pool3_1')(conv3_4)\n",
        "#     drop3_1 = Dropout(0.3, name = 'drop3_1')(pool3_1)#the 4-th block\n",
        "#     conv4_1 = Conv2D(256, kernel_size=3, activation='relu', padding='same', name = 'conv4_1')(drop3_1)\n",
        "#     conv4_1 = BatchNormalization()(conv4_1)\n",
        "#     conv4_2 = Conv2D(256, kernel_size=3, activation='relu', padding='same', name = 'conv4_2')(conv4_1)\n",
        "#     conv4_2 = BatchNormalization()(conv4_2)\n",
        "#     conv4_3 = Conv2D(256, kernel_size=3, activation='relu', padding='same', name = 'conv4_3')(conv4_2)\n",
        "#     conv4_3 = BatchNormalization()(conv4_3)\n",
        "#     conv4_4 = Conv2D(256, kernel_size=3, activation='relu', padding='same', name = 'conv4_4')(conv4_3)\n",
        "#     conv4_4 = BatchNormalization()(conv4_4)\n",
        "#     pool4_1 = MaxPooling2D(pool_size=(2,2), name = 'pool4_1')(conv4_4)\n",
        "#     drop4_1 = Dropout(0.3, name = 'drop4_1')(pool4_1) \n",
        "#     #the 5-th block\n",
        "#     conv5_1 = Conv2D(512, kernel_size=3, activation='relu', padding='same', name = 'conv5_1')(drop4_1)\n",
        "#     conv5_1 = BatchNormalization()(conv5_1)\n",
        "#     conv5_2 = Conv2D(512, kernel_size=3, activation='relu', padding='same', name = 'conv5_2')(conv5_1)\n",
        "#     conv5_2 = BatchNormalization()(conv5_2)\n",
        "#     conv5_3 = Conv2D(512, kernel_size=3, activation='relu', padding='same', name = 'conv5_3')(conv5_2)\n",
        "#     conv5_3 = BatchNormalization()(conv5_3)\n",
        "#     conv5_4 = Conv2D(512, kernel_size=3, activation='relu', padding='same', name = 'conv5_4')(conv5_3)\n",
        "#     conv5_3 = BatchNormalization()(conv5_3)\n",
        "#     pool5_1 = MaxPooling2D(pool_size=(2,2), name = 'pool5_1')(conv5_4)\n",
        "#     drop5_1 = Dropout(0.3, name = 'drop5_1')(pool5_1)#Flatten and output\n",
        "#     flatten = Flatten(name = 'flatten')(drop5_1)\n",
        "#     ouput = Dense(num_classes, activation='softmax', name = 'output')(flatten)# create model \n",
        "#     model = Model(inputs =visible, outputs = ouput)\n",
        "#     # summary layers\n",
        "#     print(model.summary())\n",
        "    \n",
        "#     return model\n"
      ],
      "metadata": {
        "id": "i92Pi-XJ94Y0"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "model=my_model()\n",
        "#summary of model \n",
        "model.summary()"
      ],
      "metadata": {
        "id": "GRvsBbBOAPlG"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#checkpoint to save best weights  \n",
        "checkpoint_filepath = 'model.h5'\n",
        "model_checkpoint_callback = tf.keras.callbacks.ModelCheckpoint(\n",
        "    filepath=checkpoint_filepath,\n",
        "    save_weights_only=True,\n",
        "    monitor='val_accuracy',\n",
        "    mode='max',\n",
        "    save_best_only=True)\n",
        "#fit our model with batch size 64 with 20 epochs \n",
        "model.fit(X_train,\n",
        "          y_train_k,\n",
        "          batch_size=64,\n",
        "          epochs=20,\n",
        "          verbose=1,\n",
        "          validation_data=(X_valid, y_valid_k),\n",
        "          shuffle=True,\n",
        "          callbacks=[model_checkpoint_callback]\n",
        "          )"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Nbvaik0Fpdx0",
        "outputId": "9497fcf9-b6c2-4331-c09b-b88e5ca517d9"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1/20\n",
            "728/728 [==============================] - 180s 228ms/step - loss: 7.4409 - accuracy: 0.5649 - val_loss: 5.9771 - val_accuracy: 0.6845\n",
            "Epoch 2/20\n",
            "728/728 [==============================] - 164s 225ms/step - loss: 4.8555 - accuracy: 0.7250 - val_loss: 3.8527 - val_accuracy: 0.7483\n",
            "Epoch 3/20\n",
            "728/728 [==============================] - 160s 220ms/step - loss: 3.0592 - accuracy: 0.7744 - val_loss: 2.3856 - val_accuracy: 0.7987\n",
            "Epoch 4/20\n",
            "728/728 [==============================] - 160s 220ms/step - loss: 1.9682 - accuracy: 0.8044 - val_loss: 1.6298 - val_accuracy: 0.8124\n",
            "Epoch 5/20\n",
            "728/728 [==============================] - 160s 219ms/step - loss: 1.3587 - accuracy: 0.8262 - val_loss: 1.2028 - val_accuracy: 0.8162\n",
            "Epoch 6/20\n",
            "728/728 [==============================] - 160s 219ms/step - loss: 1.0042 - accuracy: 0.8455 - val_loss: 1.0308 - val_accuracy: 0.8115\n",
            "Epoch 7/20\n",
            "728/728 [==============================] - 160s 219ms/step - loss: 0.8354 - accuracy: 0.8525 - val_loss: 0.8553 - val_accuracy: 0.8308\n",
            "Epoch 8/20\n",
            "728/728 [==============================] - 160s 219ms/step - loss: 0.7292 - accuracy: 0.8674 - val_loss: 0.7893 - val_accuracy: 0.8360\n",
            "Epoch 9/20\n",
            "728/728 [==============================] - 159s 219ms/step - loss: 0.6683 - accuracy: 0.8790 - val_loss: 0.7887 - val_accuracy: 0.8344\n",
            "Epoch 10/20\n",
            "728/728 [==============================] - 159s 219ms/step - loss: 0.6672 - accuracy: 0.8802 - val_loss: 0.8177 - val_accuracy: 0.8293\n",
            "Epoch 11/20\n",
            "728/728 [==============================] - 159s 219ms/step - loss: 0.6235 - accuracy: 0.8911 - val_loss: 0.7431 - val_accuracy: 0.8525\n",
            "Epoch 12/20\n",
            "728/728 [==============================] - 159s 219ms/step - loss: 0.6073 - accuracy: 0.8982 - val_loss: 0.7665 - val_accuracy: 0.8534\n",
            "Epoch 13/20\n",
            "728/728 [==============================] - 164s 225ms/step - loss: 0.5925 - accuracy: 0.9055 - val_loss: 0.7561 - val_accuracy: 0.8520\n",
            "Epoch 14/20\n",
            "728/728 [==============================] - 160s 219ms/step - loss: 0.5757 - accuracy: 0.9110 - val_loss: 0.7499 - val_accuracy: 0.8565\n",
            "Epoch 15/20\n",
            "728/728 [==============================] - 160s 219ms/step - loss: 0.5665 - accuracy: 0.9157 - val_loss: 0.7959 - val_accuracy: 0.8510\n",
            "Epoch 16/20\n",
            "728/728 [==============================] - 160s 219ms/step - loss: 0.5511 - accuracy: 0.9234 - val_loss: 0.7757 - val_accuracy: 0.8514\n",
            "Epoch 17/20\n",
            "728/728 [==============================] - 160s 219ms/step - loss: 0.5555 - accuracy: 0.9236 - val_loss: 0.8132 - val_accuracy: 0.8540\n",
            "Epoch 18/20\n",
            "728/728 [==============================] - 160s 219ms/step - loss: 0.5487 - accuracy: 0.9294 - val_loss: 0.8084 - val_accuracy: 0.8538\n",
            "Epoch 19/20\n",
            "728/728 [==============================] - 160s 219ms/step - loss: 0.5306 - accuracy: 0.9335 - val_loss: 0.8790 - val_accuracy: 0.8387\n",
            "Epoch 20/20\n",
            "728/728 [==============================] - 160s 219ms/step - loss: 0.5316 - accuracy: 0.9354 - val_loss: 0.8581 - val_accuracy: 0.8520\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#save model \n",
        "model.save('CNN-Arch.h5')"
      ],
      "metadata": {
        "id": "Ejj627GnypEz"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}